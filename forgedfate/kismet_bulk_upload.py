#!/usr/bin/env python3
"""
ForgedFate Kismet Bulk Upload Tool
Uploads all collected Kismet logs to Elasticsearch in bulk
"""

import argparse
import asyncio
import json
import logging
import os
import sqlite3
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

try:
    from elasticsearch import Elasticsearch
    from elasticsearch.helpers import bulk
except ImportError:
    print("Error: elasticsearch library not found. Install with: pip install elasticsearch")
    sys.exit(1)

class KismetBulkUploader:
    """Bulk upload all Kismet logs to Elasticsearch"""
    
    def __init__(self, es_hosts: str, username: str = "", password: str = "", 
                 index_prefix: str = "kismet", device_name: str = "unknown"):
        self.es_hosts = es_hosts
        self.username = username
        self.password = password
        self.index_prefix = index_prefix
        self.device_name = device_name
        self.es_client = None
        
        # Statistics
        self.stats = {
            'files_processed': 0,
            'documents_uploaded': 0,
            'errors': 0,
            'start_time': None,
            'data_types': {}
        }
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        

    
    def discover_log_files(self, log_directory: str = ".") -> Dict[str, List[str]]:
        """Discover all Kismet log files"""
        log_files = {
            'kismetdb': [],
            'json': [],
            'pcap': [],
            'other': []
        }
        
        search_paths = [
            log_directory,
            "/opt/kismet/logs",
            "~/.kismet",
            "./logs"
        ]
        
        for search_path in search_paths:
            path = Path(search_path).expanduser()
            if path.exists():
                self.logger.info(f"Searching for logs in: {path}")
                
                # Find .kismet files
                for kismet_file in path.glob("*.kismet"):
                    log_files['kismetdb'].append(str(kismet_file))
                
                # Find JSON files
                for json_file in path.glob("*.json"):
                    if any(x in json_file.name.lower() for x in ['kismet', 'device', 'bluetooth', 'wifi']):
                        log_files['json'].append(str(json_file))
                
                # Find PCAP files
                for pcap_file in path.glob("*.pcap*"):
                    log_files['pcap'].append(str(pcap_file))
        
        # Log discovery results
        for log_type, files in log_files.items():
            if files:
                self.logger.info(f"Found {len(files)} {log_type} files")
                for file in files[:3]:  # Show first 3
                    self.logger.info(f"  - {file}")
                if len(files) > 3:
                    self.logger.info(f"  ... and {len(files) - 3} more")
        
        return log_files
    
    def process_kismetdb_file(self, db_path: str) -> List[Dict]:
        """Extract data from .kismet database file"""
        documents = []
        
        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # Get all tables
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            self.logger.info(f"Processing {len(tables)} tables from {db_path}")
            
            for table in tables:
                try:
                    cursor.execute(f"SELECT * FROM {table} LIMIT 1000")  # Limit for bulk upload
                    rows = cursor.fetchall()
                    
                    for row in rows:
                        doc = {
                            '@timestamp': datetime.utcnow().isoformat(),
                            'source_file': os.path.basename(db_path),
                            'source_table': table,
                            'device_name': self.device_name,
                            'data_type': 'kismetdb'
                        }
                        
                        # Add all columns as fields
                        for key in row.keys():
                            value = row[key]
                            if value is not None:
                                # Handle JSON fields
                                if isinstance(value, str) and value.startswith('{'):
                                    try:
                                        doc[key] = json.loads(value)
                                    except:
                                        doc[key] = value
                                else:
                                    doc[key] = value
                        
                        documents.append(doc)
                    
                    self.stats['data_types'][f'kismetdb_{table}'] = len(rows)
                    
                except Exception as e:
                    self.logger.warning(f"Error processing table {table}: {e}")
            
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error processing kismetdb file {db_path}: {e}")
            self.stats['errors'] += 1
        
        return documents
    
    def process_json_file(self, json_path: str) -> List[Dict]:
        """Process JSON log files"""
        documents = []
        
        try:
            with open(json_path, 'r') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        data = json.loads(line)
                        
                        doc = {
                            '@timestamp': datetime.utcnow().isoformat(),
                            'source_file': os.path.basename(json_path),
                            'line_number': line_num,
                            'device_name': self.device_name,
                            'data_type': 'json_log'
                        }
                        
                        # Merge the JSON data
                        doc.update(data)
                        documents.append(doc)
                        
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"Invalid JSON at line {line_num} in {json_path}: {e}")
            
            self.stats['data_types']['json_logs'] = len(documents)
            
        except Exception as e:
            self.logger.error(f"Error processing JSON file {json_path}: {e}")
            self.stats['errors'] += 1
        
        return documents
    
    def upload_documents(self, documents: List[Dict], data_type: str) -> bool:
        """Upload documents to Elasticsearch"""
        if not documents:
            return True

        # Try multiple index patterns since user can't create new indices
        date_str = datetime.now().strftime("%Y.%m.%d")
        potential_indices = [
            f"filebeat-7.17.0-{date_str}-000001",  # Common Filebeat pattern
            f"filebeat-{date_str}",
            f"logstash-{date_str}",
            f"{self.index_prefix}-{data_type}-{date_str}",
            f"{self.index_prefix}-{date_str}",
        ]

        for index_name in potential_indices:
            try:
                self.logger.info(f"Attempting upload to index: {index_name}")

                # Prepare documents for bulk upload
                actions = []
                for doc in documents:
                    action = {
                        "_index": index_name,
                        "_source": doc
                    }
                    actions.append(action)

                # Bulk upload
                success_count, failed_items = bulk(
                    self.es_client,
                    actions,
                    chunk_size=500,
                    request_timeout=60
                )

                self.logger.info(f"âœ… Successfully uploaded {success_count} documents to {index_name}")
                self.stats['documents_uploaded'] += success_count

                if failed_items:
                    self.logger.warning(f"Failed to upload {len(failed_items)} documents")
                    self.stats['errors'] += len(failed_items)

                return True

            except Exception as e:
                self.logger.warning(f"Failed to upload to {index_name}: {str(e)[:100]}...")
                continue

        # If all indices failed
        self.logger.error(f"Failed to upload to any index. Tried: {potential_indices}")
        self.stats['errors'] += 1
        return False
    
    def run_bulk_upload(self, log_directory: str = ".") -> Dict:
        """Run the complete bulk upload process"""
        self.stats['start_time'] = time.time()
        
        self.logger.info("ðŸš€ Starting ForgedFate Kismet Bulk Upload...")
        
        # Configure Elasticsearch client (handle connection test gracefully)
        self.logger.info("Configuring Elasticsearch client for write-only access...")

        auth = None
        if self.username and self.password:
            auth = (self.username, self.password)

        try:
            self.es_client = Elasticsearch(
                [self.es_hosts],
                basic_auth=auth,
                verify_certs=False,
                ssl_show_warn=False
            )
            self.logger.info("Elasticsearch client configured successfully")
        except Exception as e:
            # Handle automatic connection test errors (expected for write-only users)
            self.logger.warning(f"Connection test failed (expected): {str(e)[:100]}...")
            self.logger.info("Proceeding with data processing anyway...")

            # Create client without triggering connection test
            self.es_client = Elasticsearch(
                [self.es_hosts],
                basic_auth=auth,
                verify_certs=False,
                ssl_show_warn=False
            )

        
        # Discover log files
        log_files = self.discover_log_files(log_directory)
        
        # Process each file type
        for kismet_file in log_files['kismetdb']:
            self.logger.info(f"Processing kismetdb file: {kismet_file}")
            documents = self.process_kismetdb_file(kismet_file)
            if documents:
                self.upload_documents(documents, 'kismetdb')
            self.stats['files_processed'] += 1
        
        for json_file in log_files['json']:
            self.logger.info(f"Processing JSON file: {json_file}")
            documents = self.process_json_file(json_file)
            if documents:
                self.upload_documents(documents, 'json')
            self.stats['files_processed'] += 1
        
        # Print final statistics
        runtime = time.time() - self.stats['start_time']
        self.logger.info("ðŸŽ‰ Bulk upload complete!")
        self.logger.info(f"Files processed: {self.stats['files_processed']}")
        self.logger.info(f"Documents uploaded: {self.stats['documents_uploaded']}")
        self.logger.info(f"Errors: {self.stats['errors']}")
        self.logger.info(f"Runtime: {runtime:.1f} seconds")
        
        return self.stats

def main():
    parser = argparse.ArgumentParser(description="ForgedFate Kismet Bulk Upload Tool")
    parser.add_argument("--es-hosts", required=True, help="Elasticsearch hosts")
    parser.add_argument("--es-username", help="Elasticsearch username")
    parser.add_argument("--es-password", help="Elasticsearch password")
    parser.add_argument("--index-prefix", default="kismet", help="Index prefix")
    parser.add_argument("--device-name", default="unknown", help="Device name")
    parser.add_argument("--log-directory", default=".", help="Directory to search for logs")
    
    args = parser.parse_args()
    
    uploader = KismetBulkUploader(
        es_hosts=args.es_hosts,
        username=args.es_username,
        password=args.es_password,
        index_prefix=args.index_prefix,
        device_name=args.device_name
    )
    
    stats = uploader.run_bulk_upload(args.log_directory)
    
    if stats['errors'] > 0:
        sys.exit(1)

if __name__ == "__main__":
    main()
